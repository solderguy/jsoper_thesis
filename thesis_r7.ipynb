{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras.preprocessing import image    \n",
    "from tensorflow.python.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import random\n",
    "from math import ceil\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#DATASET = 'flowers'\n",
    "DATASET = 'stanford'\n",
    "# DATASET = 'dogs_v2'\n",
    "AUGMENT_DATA = True\n",
    "#MODEL_NAME = 'InceptionResNetV2'  not working, doesn't like maxpool\n",
    "MODEL_NAME = 'InceptionV3'\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "RUN_MODE = 'test2'\n",
    "\n",
    "if MODEL_NAME == 'VGG16':\n",
    "    from tensorflow.python.keras.applications.vgg16 import VGG16 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'VGG19':\n",
    "    from tensorflow.python.keras.applications.vgg19 import VGG19 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'ResNet50':\n",
    "    from tensorflow.python.keras.applications.resnet50 import ResNet50 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'InceptionV3':\n",
    "    from tensorflow.python.keras.applications.inception_v3 import InceptionV3 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'Xception':\n",
    "    from tensorflow.python.keras.applications.xception import Xception as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'InceptionResNetV2':\n",
    "    from keras.applications.inception_resnet_v2 import InceptionResNetV2 as model_type, preprocess_input\n",
    "# both mobilenets run but warn about input_shape not being square.  Don't trust them\n",
    "elif MODEL_NAME == 'MobileNet':\n",
    "    from tensorflow.python.keras.applications.mobilenet import MobileNet as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'MobileNetV2':\n",
    "    from tensorflow.python.keras.applications.mobilenet_v2 import MobileNetV2 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'DenseNet201':\n",
    "    from tensorflow.python.keras.applications.densenet import DenseNet201 as model_type, preprocess_input\n",
    "elif MODEL_NAME == 'NASNetLarge':\n",
    "    from tensorflow.python.keras.applications.nasnet import NASNetLarge as model_type, preprocess_input\n",
    "\n",
    "\n",
    "#print('tensorflow version:', tensorflow.__version__)\n",
    "print('keras version:', keras.__version__)\n",
    "\n",
    "# from os import environ\n",
    "# if 'COLAB_GPU' in environ:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/gdrive')\n",
    "#     %cd '/content/gdrive/My Drive/work_dog93'\n",
    "\n",
    "if DATASET == 'stanford':\n",
    "    train_data_path = 'StanfordDogImages/train/'\n",
    "    valid_data_path = 'StanfordDogImages/valid/'\n",
    "    test_data_path = 'StanfordDogImages/test/'\n",
    "elif DATASET == 'flowers':\n",
    "    train_data_path = 'flower_split/train/'\n",
    "    valid_data_path = 'flower_split/valid/'\n",
    "    test_data_path = 'dogImages/test/'\n",
    "else:\n",
    "    print('************ incorrect dataset **********************')\n",
    "    sys.exit()\n",
    "    \n",
    "print('Data: ', DATASET)\n",
    "print('Augment Data: ', AUGMENT_DATA)\n",
    "print('Model: ', MODEL_NAME)\n",
    "print('Batch Size: ', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utiity functions\n",
    "\n",
    "def top2_acc(labels, logits): \n",
    "    return top_k_categorical_accuracy(y_true=labels, y_pred=logits, k=2)\n",
    "    \n",
    "def get_metrics():\n",
    "    if DATASET == 'flowers':\n",
    "        metrics = ['accuracy', top2_acc]\n",
    "    else:\n",
    "        metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    "    return metrics\n",
    "\n",
    "def get_storage_file_names():\n",
    "    base_name = DATASET + '_' + MODEL_NAME\n",
    "    if AUGMENT_DATA:\n",
    "        base_name += '_da'\n",
    "\n",
    "    weight_file_name = 'saved_weights/weights_' + base_name + '.hdf5'\n",
    "    model_file_name = 'saved_models/' + base_name + '_model.h5'\n",
    "    \n",
    "    return weight_file_name, model_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scratch_data():\n",
    "    if AUGMENT_DATA:\n",
    "        scratch_generator = ImageDataGenerator(rescale=1/255,\n",
    "                                                  rotation_range=10.0,\n",
    "                                                  width_shift_range=0.1,\n",
    "                                                  height_shift_range=0.1,\n",
    "                                                  shear_range=0.05,\n",
    "                                                  zoom_range=0.1,\n",
    "                                                  horizontal_flip=True,\n",
    "                                                  vertical_flip=False,\n",
    "                                                  fill_mode=\"reflect\")\n",
    "    else:\n",
    "        scratch_generator = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    scratch_train_batches = scratch_generator.flow_from_directory(train_data_path,\n",
    "                                                       target_size=(350, 350),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       shuffle=True,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "    scratch_valid_batches = scratch_generator.flow_from_directory(valid_data_path,\n",
    "                                                           target_size=(350, 350),\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           shuffle=True,\n",
    "                                                           class_mode='categorical')\n",
    "    return scratch_train_batches, scratch_valid_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The architecture is based off the VGG architecture, but with just 1 conv layer for each block.\n",
    "\n",
    "def build_scratch_model(num_classes):\n",
    "    model = Sequential()\n",
    "    # Conv layer1\n",
    "    model.add(Conv2D(32, 3, strides=(1,1), padding='same', activation='relu', input_shape=(350,350,3)))\n",
    "    model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "    # Conv layer2\n",
    "    model.add(Conv2D(64, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "    # Conv layer3\n",
    "    model.add(Conv2D(128, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "    # Conv layer4\n",
    "    model.add(Conv2D(256, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "    # Conv layer5\n",
    "    model.add(Conv2D(256, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "    #Flatten Layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #Fully Connected Layer 2\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=get_metrics())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_scratch():\n",
    "    train_batches, valid_batches = make_scratch_data()   \n",
    "    num_classes = len(set(train_batches.classes)) \n",
    "\n",
    "    train_steps = ceil(len(train_batches.filenames) / BATCH_SIZE)\n",
    "    valid_steps = ceil(len(valid_batches.filenames) / BATCH_SIZE)\n",
    "\n",
    "    weight_file_name, model_file_name = get_storage_file_names()\n",
    "    checkpointer = ModelCheckpoint(filepath=weight_file_name, \n",
    "                                   monitor='val_acc',\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)\n",
    "\n",
    "    model = build_scratch_model(num_classes)\n",
    "    history = model.fit_generator(train_batches, \n",
    "                        steps_per_epoch = train_steps, \n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[checkpointer], \n",
    "                        verbose=1, \n",
    "                        validation_data=valid_batches,\n",
    "                        validation_steps=valid_steps)\n",
    "    print('first 25')\n",
    "    print('\\n', history.history)\n",
    "    model.save(model_file_name)\n",
    "    history = model.fit_generator(train_batches, \n",
    "                        steps_per_epoch = train_steps, \n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[checkpointer], \n",
    "                        verbose=1, \n",
    "                        validation_data=valid_batches,\n",
    "                        validation_steps=valid_steps)\n",
    "    print('2nd 25')\n",
    "\n",
    "    print('\\n', history.history)\n",
    "    model.save(model_file_name)\n",
    "    history = model.fit_generator(train_batches, \n",
    "                        steps_per_epoch = train_steps, \n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[checkpointer], \n",
    "                        verbose=1, \n",
    "                        validation_data=valid_batches,\n",
    "                        validation_steps=valid_steps)\n",
    "    print('3nd 25')\n",
    "    print('\\n', history.history)\n",
    "\n",
    "    \n",
    "    return history\n",
    "\n",
    "if MODEL_NAME == 'scratch':\n",
    "    history = run_from_scratch()\n",
    "    print('\\n\\n************************************ done running')\n",
    "    import sys\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagenet_prep(image_tensor):\n",
    "    '''\n",
    "\n",
    "        Takes the image tensors for one image. Applies the preprocessing need for the Resnet and VGG models.\n",
    "        \n",
    "        Inputs: Image tensor of shape (width, height, colorchannels)\n",
    "        \n",
    "        Returns: Tensor of same shape with color channels flipped and values centered around zero for \n",
    "        each color channel.\n",
    "    '''\n",
    "    # imagenet averages for RGB\n",
    "    image_net_mean = np.array([103.939,116.779,123.68])\n",
    "    image_tensor -= image_net_mean\n",
    "    \n",
    "    # Flip color channels\n",
    "    image_tensor = image_tensor[:, :, ::-1]\n",
    "    return image_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches_from_gen():\n",
    "    if AUGMENT_DATA:\n",
    "        generator = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                               rescale=1.,\n",
    "                                               rotation_range=10.0,\n",
    "                                               width_shift_range=0.1,\n",
    "                                               height_shift_range=0.1,\n",
    "                                               shear_range=0.05,\n",
    "                                               zoom_range=0.1,\n",
    "                                               horizontal_flip=True,\n",
    "                                               vertical_flip=False,\n",
    "                                               fill_mode=\"reflect\")\n",
    "    else:\n",
    "        generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "\n",
    "    train_batches = generator.flow_from_directory(train_data_path,\n",
    "                                                  target_size=(350, 350),\n",
    "                                                  #target_size=(331, 331),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  shuffle=True,\n",
    "                                                  class_mode='categorical')\n",
    " \n",
    "    valid_batches = generator.flow_from_directory(valid_data_path,\n",
    "                                                  target_size=(299, 299),\n",
    "                                                  #target_size=(350, 350),\n",
    "                                                  #target_size=(331, 331), #NAS\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "    return train_batches, valid_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_model(train_batches):\n",
    "    num_classes = len(set(train_batches.classes))\n",
    "    \n",
    "    base_model = model_type(include_top=False, weights='imagenet')\n",
    "    first_extra_layer = 1 + len(base_model.layers)\n",
    "    #print('first extra layer', first_extra_layer)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)   \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    preds = Dense(num_classes, activation='softmax')(x)   \n",
    "    model = Model(inputs=base_model.input, outputs=preds)  \n",
    "    \n",
    "#     for i, layer in enumerate(model.layers):\n",
    "#         print(i, layer)\n",
    "\n",
    "    for layer in model.layers[:first_extra_layer]:\n",
    "      layer.trainable=False\n",
    "    for layer in model.layers[first_extra_layer:]:\n",
    "      layer.trainable=True\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretrained_model(saved_weights_name, saved_model_name):\n",
    "    train_batches, valid_batches = get_batches_from_gen()\n",
    "    model = build_transfer_model(train_batches)\n",
    "\n",
    "    model.compile(optimizer=Adam(), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=get_metrics())\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=saved_weights_name, \n",
    "                                   monitor='val_acc',\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)\n",
    "    if RUN_MODE == 'train':\n",
    "        results = model.fit_generator(generator=train_batches,\n",
    "                   steps_per_epoch=ceil(train_batches.n/BATCH_SIZE),\n",
    "                   validation_data=valid_batches,\n",
    "                   validation_steps = ceil(valid_batches.n/BATCH_SIZE),\n",
    "                   callbacks=[checkpointer], \n",
    "                   verbose=1,\n",
    "                   epochs=EPOCHS)\n",
    "    \n",
    "        print(results.history)\n",
    "        model.save(saved_model_name)\n",
    "       \n",
    "#         dependencies = {\n",
    "#             'top2_acc': top2_acc\n",
    "#         }\n",
    "\n",
    "#         model = load_model('./saved_models/t50_stanford_InceptionV3_ad.h5',\n",
    "#                           custom_objects=dependencies)\n",
    "# #         model = load_model('./saved_models/stanford_InceptionV3_ad.h5')\n",
    "\n",
    "    \n",
    "# #         results = model.fit_generator(generator=train_batches,\n",
    "# #                    steps_per_epoch=ceil(train_batches.n/BATCH_SIZE),\n",
    "# #                    validation_data=valid_batches,\n",
    "# #                    validation_steps = ceil(valid_batches.n/BATCH_SIZE),\n",
    "# #                    callbacks=[checkpointer], \n",
    "# #                    verbose=1,\n",
    "# #                    epochs=EPOCHS)\n",
    "    \n",
    "# #         print(results.history)\n",
    "# #         model.save(saved_model_name)\n",
    "\n",
    "    \n",
    "    \n",
    "# #         model = load_model('./saved_models/t50_flowers_Xception_ad.h5',\n",
    "# #                           custom_objects=dependencies)\n",
    "#           # 127 Xception?   295 Inception\n",
    "#         for layer in model.layers[:295]:   \n",
    "#             layer.trainable=False\n",
    "#         for layer in model.layers[295:]:\n",
    "#             layer.trainable=True\n",
    "            \n",
    "#         model.compile(optimizer=SGD(lr=0.0001, \n",
    "#                                     momentum=0.9),  \n",
    "#                                     loss='categorical_crossentropy', \n",
    "#                                     metrics=get_metrics())\n",
    "\n",
    "#         results = model.fit_generator(generator=train_batches,\n",
    "#                    steps_per_epoch=ceil(train_batches.n/BATCH_SIZE),\n",
    "#                    validation_data=valid_batches,\n",
    "#                    validation_steps = ceil(valid_batches.n/BATCH_SIZE),\n",
    "#                    callbacks=[checkpointer], \n",
    "#                    verbose=1,\n",
    "#                    epochs=EPOCHS)\n",
    "#         print(results.history)\n",
    "#         model.save(saved_model_name)\n",
    "\n",
    "        \n",
    "    elif RUN_MODE == 'test':\n",
    "        #model = load_model('./saved_models/flowers_InceptionV3_ad.h5')\n",
    "        model.load_weights(\"./saved_weights/final_stanford_InceptionV3_ad_best.hdf5\")\n",
    "        score = model.evaluate_generator(generator=valid_batches,\n",
    "                                     steps=ceil(valid_batches.n/BATCH_SIZE))\n",
    "#         print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "#         print(model.metrics_names)\n",
    "        results = {}\n",
    "        results[model.metrics_names[1]] = score[1]*100\n",
    "        results[model.metrics_names[2]] = score[2]*100\n",
    "        print('results: ', results)\n",
    "        \n",
    "    elif RUN_MODE == 'test2':\n",
    "        \n",
    "        def translate_to_imagenet_values(key):\n",
    "            key = int(key)\n",
    "            if key > 272:\n",
    "                return key + 155 # dingo, dhole, African hunting dog\n",
    "            else:\n",
    "                return key + 151\n",
    "            \n",
    "        class_names = {}  \n",
    "        for folder in listdir(valid_data_path):\n",
    "            key, value = folder.split('.')\n",
    "            key = translate_to_imagenet_values(key.strip())\n",
    "            class_names[key] = value.strip()\n",
    "            \n",
    "        model = model_type(include_top=True, weights='imagenet')\n",
    "        results = model.predict_generator(generator=valid_batches,\n",
    "                                         steps=ceil(valid_batches.n/BATCH_SIZE))\n",
    "        y_pred = np.argmax(results, axis = 1)   \n",
    "        y_true = valid_batches.classes\n",
    "        predict_count, mispredict_count, non_dog_count = 0, 0, 0\n",
    "        \n",
    "        for i in range(len(y_true)):\n",
    "            predict_count += 1\n",
    "            single_prediction = y_pred[i]\n",
    "            single_actual = translate_to_imagenet_values(y_true[i])\n",
    "            if single_prediction != single_actual:\n",
    "                mispredict_count += 1\n",
    "                if single_prediction not in class_names:\n",
    "                    non_dog_count += 1\n",
    "            \n",
    "        print('accuracy: ', 1 - mispredict_count / predict_count)\n",
    "        print('non-dog error percent', non_dog_count / mispredict_count)\n",
    "        results = 1                   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run():\n",
    "    base_name = DATASET + '_' + MODEL_NAME\n",
    "    if AUGMENT_DATA:\n",
    "        base_name = base_name + '_ad'\n",
    "    result_file_name = base_name + '_results.h5'\n",
    "    saved_weights_name = 'saved_weights/' + base_name + '_best.hdf5'\n",
    "    saved_model_name = 'saved_models/' + base_name + '.h5'\n",
    "    !touch result_file_name && rm result_file_name\n",
    "    !touch saved_weights_name && rm saved_weights_name\n",
    "  \n",
    "    history = run_pretrained_model(saved_weights_name, saved_model_name)\n",
    "#     print(history.history)\n",
    "#     df = pd.DataFrame.from_dict(history.history)\n",
    "#     df.to_hdf(result_file_name, 'df', format='t')\n",
    "    \n",
    "full_run()\n",
    "print('\\n\\n************************************ done running')\n",
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
